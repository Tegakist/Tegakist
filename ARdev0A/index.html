<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WebAR with ORB/SIFT</title>
    <style>
        body { margin: 0; }
        canvas { display: block; }
    </style>
    <script async src="https://docs.opencv.org/4.x/opencv.js" type="text/javascript"></script>
</head>
<body>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script>
        // ディスクリプタファイルを読み込む
        fetch('descriptors.npy')
            .then(response => response.arrayBuffer())
            .then(buffer => {
                const descriptors = new Float32Array(buffer);

                // 画像認識とWebARの実装
                console.log('Descriptors loaded:', descriptors);

                // OpenCV.jsの初期化
                cv['onRuntimeInitialized'] = () => {
                    // カメラ映像を取得し、特徴点を検出して比較する処理を開始
                    const video = document.createElement('video');
                    video.setAttribute('autoplay', '');
                    video.setAttribute('muted', '');
                    video.setAttribute('playsinline', '');
                    document.body.appendChild(video);

                    navigator.mediaDevices.getUserMedia({ video: true })
                        .then(stream => {
                            video.srcObject = stream;
                            video.onloadedmetadata = () => {
                                video.play();
                                detectAndMatchFeatures(video, descriptors);
                            };
                        })
                        .catch(err => {
                            console.error('Error accessing camera:', err);
                        });

                    function detectAndMatchFeatures(video, descriptors) {
                        const src = new cv.Mat(video.height, video.width, cv.CV_8UC4);
                        const gray = new cv.Mat();
                        const cap = new cv.VideoCapture(video);

                        function processVideo() {
                            cap.read(src);
                            cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY, 0);

                            // ORBを使用して特徴点を検出
                            const orb = new cv.ORB();
                            const keypoints = new cv.KeyPointVector();
                            const descriptor = new cv.Mat();
                            orb.detectAndCompute(gray, new cv.Mat(), keypoints, descriptor);

                            // ディスクリプタと比較
                            const bf = new cv.BFMatcher(cv.NORM_HAMMING, true);
                            const matches = new cv.DMatchVector();
                            bf.match(descriptor, descriptors, matches);

                            // マッチング結果を表示
                            console.log('Matches:', matches.size());

                            // 次のフレームを処理
                            requestAnimationFrame(processVideo);
                        }

                        // 最初のフレームを処理
                        requestAnimationFrame(processVideo);
                    }
                };
            });

        // THREE.jsの基本的なセットアップ
        const scene = new THREE.Scene();
        const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
        const renderer = new THREE.WebGLRenderer();
        renderer.setSize(window.innerWidth, window.innerHeight);
        document.body.appendChild(renderer.domElement);

        const geometry = new THREE.BoxGeometry();
        const material = new THREE.MeshBasicMaterial({ color: 0x00ff00 });
        const cube = new THREE.Mesh(geometry, material);
        scene.add(cube);

        camera.position.z = 5;

        function animate() {
            requestAnimationFrame(animate);
            renderer.render(scene, camera);
        }

        animate();
    </script>
</body>
</html>
